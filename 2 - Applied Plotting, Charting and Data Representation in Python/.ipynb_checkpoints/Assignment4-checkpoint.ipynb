{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import bs4\n",
    "# URL = 'https://en.wikipedia.org/wiki/2020_Pacific_typhoon_season'\n",
    "# page = requests.get(URL)\n",
    "\n",
    "# soup = BeautifulSoup(page.content, 'html.parser')\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import bs4\n",
    "# import numpy as np \n",
    "\n",
    "# years = np.arange(2018,2019)\n",
    "\n",
    "# def names(tags):\n",
    "#     return [t.name for t in tags]\n",
    "\n",
    "# for year in years:\n",
    "#     URL = f'https://en.wikipedia.org/wiki/{year}_Pacific_typhoon_season'\n",
    "#     page = requests.get(URL)\n",
    "\n",
    "#     soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "#     all_tags = [c for c in soup.find(class_='mw-parser-output') if isinstance(c, bs4.Tag)]    \n",
    "\n",
    "#     keys = ['name','jma_cat','sshws_cat']\n",
    "#     groups = []\n",
    "\n",
    "#     i = 0\n",
    "\n",
    "#     row_list = []\n",
    "#     print(f'Scraping {year}:::')\n",
    "#     try:\n",
    "#         while i < len(all_tags):\n",
    "#             group = all_tags[i:i + 2]\n",
    "#             if names(group) == ['h3', 'table']:\n",
    "#                 print(f'\\t{group}')\n",
    "#                 row = {}\n",
    "#                 row['name'] = group[0].find(class_='mw-headline').text\n",
    "#                 print(f\"GROUP3::: {group[1]['class']}\")\n",
    "#                 if group[1]['class'] == ['infobox']:\n",
    "#                     categories = group[1].findAll('tr')\n",
    "#                     row['jma_cat'] = categories[0].td.text\n",
    "#                     row['sshws_cat'] = categories[1].td.text\n",
    "#                 else:\n",
    "#                     categories = all_tags[i+2].findAll('tr')\n",
    "#                     print(f'sadasdas{all_tags[i+2]}')\n",
    "#                     row['jma_cat'] = categories[0].td.text\n",
    "#                     row['sshws_cat'] = categories[1].td.text\n",
    "#                     i+=1\n",
    "\n",
    "#                 row_list.append(row)\n",
    "#             i += 1\n",
    "\n",
    "\n",
    "#     finally:\n",
    "#         with open(f'{year}_pacific_storms.csv', 'w', newline='')  as output_file:\n",
    "#             dict_writer = csv.DictWriter(output_file, keys)\n",
    "#             dict_writer.writeheader()\n",
    "#             dict_writer.writerows(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unicodedata\n",
    "\n",
    "# import csv\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import bs4\n",
    "# import numpy as np \n",
    "# import json\n",
    "\n",
    "# years = np.arange(1970,2021)\n",
    "\n",
    "# for year in years:\n",
    "#     URL = f'https://en.wikipedia.org/wiki/{year}_Pacific_typhoon_season'\n",
    "#     page = requests.get(URL)\n",
    "\n",
    "#     soup = BeautifulSoup(page.content, 'html.parser')\n",
    "#     h3_storms = soup.findAll('h3')\n",
    "#     yearly_data = []\n",
    "#     for h3_storm in h3_storms:\n",
    "#         headline_tag = h3_storm.find(class_='mw-headline')\n",
    "#         categories = []\n",
    "#         if headline_tag:\n",
    "#             itr = h3_storm\n",
    "#             while True:\n",
    "#                 itr = itr.findNextSibling()\n",
    "# #                 print(f'{itr}\\n\\n')\n",
    "#                 if itr is None or not itr.has_attr('class'): \n",
    "#                     break\n",
    "#                 elif itr and itr.name == 'table' and 'infobox' in itr['class']:\n",
    "#                     trs = itr.tbody.findAll('tr')\n",
    "#                     for tr in trs:\n",
    "#                         if tr.td.findChildren() == []:\n",
    "#                             categories.append(unicodedata.normalize(\"NFKD\",tr.td.text))\n",
    "#                         else: \n",
    "#                             break\n",
    "# #                     print(headline_tag.text,categories)\n",
    "#                     yearly_data.append(dict(name=headline_tag.text,categories=categories))\n",
    "#                     break\n",
    "#     with open(f'datasets/{year}_Pacific_typhoon_season.json','w') as outfile:\n",
    "#         print(f'{year}={len(yearly_data)}')\n",
    "#         json.dump(yearly_data,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c1681c2d6015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'datasets/1970_Pacific_typhoon_season.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1970\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows)\u001b[0m\n\u001b[1;32m    616\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py36/lib/python3.6/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1119\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "years = np.arange(1971,2021)\n",
    "\n",
    "\n",
    "df = pd.read_json(f'datasets/1970_Pacific_typhoon_season.json')\n",
    "df['year'] = 1970\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    tmp_df = pd.read_json(f'datasets/{year}_Pacific_typhoon_season.json')\n",
    "    tmp_df['year'] = year\n",
    "    df = pd.concat([df,tmp_df])\n",
    "df = df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's check the categories that  we have\n",
    "# They all look valid except \n",
    "#    1. Monsoon depression\n",
    "#    2. Vamco\n",
    "set(itertools.chain.from_iterable(df['categories']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['name'] == 'Typhoon Vamco (Ulysses)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Upon further investigation, looks like the scraped value from Typhoon Vamco\n",
    "# is wrong because it's still active (as of Nov. 14, 2020)\n",
    "print(df[df['categories'].apply(lambda row_categories: 'Vamco' in row_categories)])\n",
    "\n",
    "# Looks like Monsoon depression should not be there. So let's just remove it. This will\n",
    "# just confuse the dataset\n",
    "print(df[df['categories'].apply(lambda row_categories: 'Monsoon depression' in row_categories)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[[1636],'categories'] = pd.Series([['Category 2 typhoon (SSHWS)', 'Typhoon  (JMA)']],index=[1636])\n",
    "df.loc[[1595],'categories'] = pd.Series([['Tropical depression (JMA)']],index=[1595])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['year'] != 2020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def restructure_categories(row):\n",
    "    categories = row['categories']\n",
    "    for category in categories:\n",
    "        pattern = r'(?P<Intensity>[A-Za-z0-9 ]+) \\((?P<Station>[A-Z]+)\\)'\n",
    "        result = re.search(pattern,category)\n",
    "        row[result.group('Station')] = result.group('Intensity')\n",
    "    return row\n",
    "            \n",
    "def forward_fill(row):\n",
    "    if pd.isna(row['SSHWS']):\n",
    "        row['SSHWS'] = row['JMA']\n",
    "    return row\n",
    "    \n",
    "# df.apply(lambda row: row['new'] = 'a')\n",
    "cleaned_df = (df.apply(restructure_categories,axis=1)\n",
    "   .dropna(subset=['JMA'])\n",
    "   .apply(forward_fill,axis=1)\n",
    "   .set_index('year'))\n",
    "\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_df['SSHWS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '(?P<Int_Name>[A-Za-z ]+) ?(?P<Local_Name>\\([A-Z][a-z]+\\))?'\n",
    "\n",
    "names = cleaned_df['name'].str.extract(pattern)\n",
    "names['Int_Name'] = names['Int_Name'].apply(lambda name: name.split()[-1])\n",
    "names['Local_Name'] = names['Local_Name'].apply(lambda name: re.sub('[()]','',str(name)))\n",
    "\n",
    "cleaned_df['Int_Name'] = names['Int_Name']\n",
    "cleaned_df['Local_Name'] = names['Local_Name']\n",
    "\n",
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_df['Local_Name'] = cleaned_df['Local_Name'].replace('nan',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_df.groupby(level=0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.cut(cleaned_df,bins=[(1970,1980),(1980,1990),(1990,2020)])\n",
    "\n",
    "\n",
    "bins = pd.IntervalIndex.from_tuples([(dec,dec+10) for dec in np.arange(1969,2011,10)])\n",
    "cleaned_df['decade'] = pd.cut(cleaned_df.index.values,bins=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.set_index('decade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = cleaned_df.groupby(cleaned_df.index).agg({'name':len})\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pivot = pd.pivot_table(cleaned_df[['name','Local_Name','SSHWS']],index=['decade','SSHWS'],aggfunc='count')\n",
    "pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style('white')\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "cat1_ty = pivot.xs(key='Category 1 typhoon',level=1)['name'].values\n",
    "cat2_ty = pivot.xs(key='Category 2 typhoon',level=1)['name'].values \n",
    "cat3_ty = pivot.xs(key='Category 3 typhoon',level=1)['name'].values\n",
    "cat4_ty = pivot.xs(key='Category 4 typhoon',level=1)['name'].values\n",
    "cat4_sty = pivot.xs(key='Category 4 super typhoon',level=1)['name'].values\n",
    "cat5_sty = pivot.xs(key='Category 5 super typhoon',level=1)['name'].values \n",
    "ts = pivot.xs(key='Tropical storm',level=1)['name'].values\n",
    "td = pivot.xs(key='Tropical depression',level=1)['name'].values\n",
    "\n",
    "x = ['1970s','1980s','1990s','2000s','2010s']\n",
    "\n",
    "p8 = plt.bar(x,cat5_sty,color='#ff6060',label='Cat. 5 Super Typhoon',linewidth=0)\n",
    "p7 = plt.bar(x,cat4_sty,bottom=cat5_sty,color='#ff8f20',linewidth=0)\n",
    "p6 = plt.bar(x,cat4_ty,bottom=cat5_sty+cat4_sty,color='#ff8f20',label='Cat. 4 Typhoon',linewidth=0)\n",
    "p5 = plt.bar(x,cat3_ty,bottom=cat5_sty+cat4_sty+cat4_ty,color='#ffc140',label='Cat. 3 Typhoon',linewidth=0)\n",
    "p4 = plt.bar(x,cat2_ty,bottom=cat5_sty+cat4_sty+cat4_ty+cat3_ty,color='#ffe775',label='Cat. 2 Typhoon',linewidth=0)\n",
    "p3 = plt.bar(x,cat1_ty,bottom=cat5_sty+cat4_sty+cat4_ty+cat3_ty+cat2_ty,color='#ffffcc',label='Cat. 1 Typhoon',linewidth=0)\n",
    "p2 = plt.bar(x,ts,bottom=cat5_sty+cat4_sty+cat4_ty+cat3_ty+cat2_ty+cat1_ty,color='#00faf4',label='Tropical Storm',linewidth=0)\n",
    "p2 = plt.bar(x,td,bottom=cat5_sty+cat4_sty+cat4_ty+cat3_ty+cat2_ty+cat1_ty+ts,color='#5ebaff',label='Tropical Depression',linewidth=0)\n",
    "\n",
    "plt.xlabel('Decade')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Western Pacific Ocean Storm Frequency in the past 50 years\\n',fontdict={'size':15})\n",
    "\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "plt.gca().legend(handles[::-1], labels[::-1], title='Storm Intensity (SSHWS)',bbox_to_anchor=(1, 0.95),frameon=False)\n",
    "spines = plt.gca().spines\n",
    "\n",
    "spines['right'].set_visible(False)\n",
    "spines['top'].set_visible(False)\n",
    "spines['left'].set_color([0,0,0,0.5])\n",
    "spines['bottom'].set_color([0,0,0,0.5])\n",
    "\n",
    "plt.gca().tick_params(axis='both', which='major',labelcolor=[0,0,0,0.75])\n",
    "plt.gca().tick_params(axis='y', which='both',left=True)\n",
    "\n",
    "plt.savefig('assignment4.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
